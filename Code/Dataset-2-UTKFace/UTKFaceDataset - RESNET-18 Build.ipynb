{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c66c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = 'D:/ConcordiaU/Winter 23/AI/Project/Datasets/2. UTKFace/FinalDataset/train/23To28/23_0_0_20170111181750321.jpg.chip.jpg'\n",
    "img = Image.open(img_path)\n",
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecea87b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfefbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import cv2\n",
    "import torch,torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d62b2",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2de5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "train_dir = 'D:/ConcordiaU/Winter 23/AI/Project/Datasets/2. UTKFace/FinalDataset/train/'\n",
    "test_dir = 'D:/ConcordiaU/Winter 23/AI/Project/Datasets/2. UTKFace/FinalDataset/test/'\n",
    "val_dir = 'D:/ConcordiaU/Winter 23/AI/Project/Datasets/2. UTKFace/FinalDataset/val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1357fc",
   "metadata": {},
   "source": [
    "### Transforms Normalizd(Mean/Std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed5108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([    \n",
    "    transforms.Resize((224, 224)),    ##Original Size 200*200\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #Imagenet pretrianed model specific to the normalization technique\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## For Attempt 1 and 2.\n",
    "# transforms = transforms.Compose([    \n",
    "#     transforms.Resize((224, 224)),    ##Original Size 200*200\n",
    "#     transforms.ToTensor(),    \n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #Imagenet pretrianed model specific to the normalization technique\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89746d",
   "metadata": {},
   "source": [
    "## Need to try with Data Aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f9d70",
   "metadata": {},
   "source": [
    "## Need to try Gaussian Blur and Laplacian filter Thres(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45976517",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transform=transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169a694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdaa8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e2523",
   "metadata": {},
   "source": [
    "### RESNET-18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f83f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suppu\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\suppu\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False, num_classes=4)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d550da",
   "metadata": {},
   "source": [
    "### Third Try - Includes Graphs(Train acc and Loss + ROC Curve) Added Early Stop and Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a403350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb1ade0b",
   "metadata": {},
   "source": [
    "### Second Try - Includes Graphs(Train acc and Loss + ROC Curve)  \n",
    "> Fail -Over fitting.\n",
    "> also forgot the val loss line :)\n",
    ">>30Epoch Only!!!!!!!!!\n",
    ">>Saving model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f44ec",
   "metadata": {},
   "source": [
    "`num_epochs = 30\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset) #Val loss and acc for each epoch\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Validation Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "# Plot training accuracy and loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "`\n",
    "\n",
    "\n",
    "\n",
    "###### Output\n",
    "<!-- # # -----------------------\n",
    "Epoch 1/30, Train Loss: 1.1304, Train Accuracy: 49.18%, Validation Accuracy: 53.84%\n",
    "Epoch 2/30, Train Loss: 0.9259, Train Accuracy: 58.66%, Validation Accuracy: 59.71%\n",
    "Epoch 3/30, Train Loss: 0.8451, Train Accuracy: 62.45%, Validation Accuracy: 43.79%\n",
    "Epoch 4/30, Train Loss: 0.7795, Train Accuracy: 65.59%, Validation Accuracy: 58.11%\n",
    "Epoch 5/30, Train Loss: 0.7322, Train Accuracy: 67.85%, Validation Accuracy: 64.95%\n",
    "Epoch 6/30, Train Loss: 0.6818, Train Accuracy: 69.53%, Validation Accuracy: 67.10%\n",
    "Epoch 7/30, Train Loss: 0.6331, Train Accuracy: 72.07%, Validation Accuracy: 63.13%\n",
    "Epoch 8/30, Train Loss: 0.5764, Train Accuracy: 74.96%, Validation Accuracy: 65.75%\n",
    "Epoch 9/30, Train Loss: 0.5182, Train Accuracy: 77.99%, Validation Accuracy: 65.67%\n",
    "Epoch 10/30, Train Loss: 0.4507, Train Accuracy: 81.03%, Validation Accuracy: 64.61%\n",
    "Epoch 11/30, Train Loss: 0.3646, Train Accuracy: 85.06%, Validation Accuracy: 66.39%\n",
    "Epoch 12/30, Train Loss: 0.2870, Train Accuracy: 88.67%, Validation Accuracy: 66.39%\n",
    "Epoch 13/30, Train Loss: 0.2235, Train Accuracy: 91.75%, Validation Accuracy: 66.26%\n",
    "Epoch 14/30, Train Loss: 0.1767, Train Accuracy: 93.67%, Validation Accuracy: 65.75%\n",
    "Epoch 15/30, Train Loss: 0.1343, Train Accuracy: 95.73%, Validation Accuracy: 67.31%\n",
    " -->\n",
    "\n",
    "\n",
    "> Stopped Training as it is overfitting.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad839fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set and ROC curve\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # For ROC curve\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(outputs[:, 1].cpu().numpy())  # Probability of positive class\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'face_age_detection_resnet18.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d21a1",
   "metadata": {},
   "source": [
    "### First Try  - Just 10 epochs and Transforms Normalizd(Mean/Std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f02557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.1079, Training Accuracy: 49.63%, Validation Loss: 0.9816, Validation Accuracy: 54.05%\n",
      "Epoch 2, Training Loss: 0.9066, Training Accuracy: 59.66%, Validation Loss: 0.9381, Validation Accuracy: 56.93%\n",
      "Epoch 3, Training Loss: 0.8149, Training Accuracy: 63.81%, Validation Loss: 0.9157, Validation Accuracy: 59.80%\n",
      "Epoch 4, Training Loss: 0.7575, Training Accuracy: 66.44%, Validation Loss: 0.9113, Validation Accuracy: 58.91%\n",
      "Epoch 5, Training Loss: 0.7069, Training Accuracy: 69.15%, Validation Loss: 1.1559, Validation Accuracy: 55.19%\n",
      "Epoch 6, Training Loss: 0.6560, Training Accuracy: 71.23%, Validation Loss: 1.0346, Validation Accuracy: 57.22%\n",
      "Epoch 7, Training Loss: 0.5984, Training Accuracy: 73.78%, Validation Loss: 0.8142, Validation Accuracy: 64.48%\n",
      "Epoch 8, Training Loss: 0.5396, Training Accuracy: 76.87%, Validation Loss: 0.8305, Validation Accuracy: 66.77%\n",
      "Epoch 9, Training Loss: 0.4718, Training Accuracy: 80.16%, Validation Loss: 0.8636, Validation Accuracy: 64.32%\n",
      "Epoch 10, Training Loss: 0.3927, Training Accuracy: 83.87%, Validation Loss: 1.1208, Validation Accuracy: 61.57%\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         _, predicted = torch.max(outputs.data, 1) #Train acc\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         train_loss += loss.item() * labels.size(0) #loss\n",
    "\n",
    "#     train_loss /= len(train_loader.dataset) #Train loss and acc for each epoch\n",
    "#     train_accuracy = 100 * correct / total\n",
    "    \n",
    "#     # evaluate on validation set\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         val_loss = 0\n",
    "#         for images, labels in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "#             _, predicted = torch.max(outputs.data, 1) #validation accuracy\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss /= len(val_loader.dataset) #Val loss and acc for each epoch\n",
    "#         val_accuracy = 100 * correct / total\n",
    "        \n",
    "#     print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0f36f",
   "metadata": {},
   "source": [
    "#### Output-1\n",
    " - Epoch 1, Training Loss: 1.1079, Training Accuracy: 49.63%, Validation Loss: 0.9816, Validation Accuracy: 54.05%\n",
    " - Epoch 2, Training Loss: 0.9066, Training Accuracy: 59.66%, Validation Loss: 0.9381, Validation Accuracy: 56.93%\n",
    " - Epoch 3, Training Loss: 0.8149, Training Accuracy: 63.81%, Validation Loss: 0.9157, Validation Accuracy: 59.80%\n",
    " - Epoch 4, Training Loss: 0.7575, Training Accuracy: 66.44%, Validation Loss: 0.9113, Validation Accuracy: 58.91%\n",
    " - Epoch 5, Training Loss: 0.7069, Training Accuracy: 69.15%, Validation Loss: 1.1559, Validation Accuracy: 55.19%\n",
    " - Epoch 6, Training Loss: 0.6560, Training Accuracy: 71.23%, Validation Loss: 1.0346, Validation Accuracy: 57.22%\n",
    " - Epoch 7, Training Loss: 0.5984, Training Accuracy: 73.78%, Validation Loss: 0.8142, Validation Accuracy: 64.48%\n",
    " - Epoch 8, Training Loss: 0.5396, Training Accuracy: 76.87%, Validation Loss: 0.8305, Validation Accuracy: 66.77%\n",
    " - Epoch 9, Training Loss: 0.4718, Training Accuracy: 80.16%, Validation Loss: 0.8636, Validation Accuracy: 64.32%\n",
    " - Epoch 10, Training Loss: 0.3927, Training Accuracy: 83.87%, Validation Loss: 1.1208, Validation Accuracy: 61.57%\n",
    " \n",
    " \n",
    " \n",
    " - Test Accuracy: 61.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10ae68dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.22%\n"
     ]
    }
   ],
   "source": [
    "# # Test the model on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
